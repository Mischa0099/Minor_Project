# AI Provider Setup Guide

The application now supports multiple AI providers with automatic priority-based selection. All providers deliver **personalized, non-hardcoded responses** based on user profiles.

## Priority Order (Automatic Selection)

1. **OpenAI** (Recommended - Most Stable) ‚≠ê
2. **Ollama** (Free, Local, No API Key)
3. **Gemini** (Less Stable)
4. **Local Transformers** (Basic Fallback)

The system automatically uses the first available provider in this order.

---

## Option 1: OpenAI (RECOMMENDED - Most Stable) ‚≠ê

### Setup
1. Get API key from: https://platform.openai.com/api-keys
2. Set environment variable:
   ```bash
   # Windows PowerShell
   $env:OPENAI_API_KEY="sk-your-key-here"
   
   # Linux/Mac
   export OPENAI_API_KEY="sk-your-key-here"
   
   # Or in .env file
   OPENAI_API_KEY=sk-your-key-here
   ```

3. (Optional) Choose model:
   ```bash
   OPENAI_MODEL=gpt-4          # Better quality, more expensive
   OPENAI_MODEL=gpt-3.5-turbo  # Default, good balance
   ```

### Pros
- ‚úÖ Most stable and reliable
- ‚úÖ Excellent personalization
- ‚úÖ Fast responses
- ‚úÖ Industry standard

### Cons
- ‚ö†Ô∏è Requires paid API key (pay-per-use)
- ‚ö†Ô∏è Costs ~$0.002 per request (GPT-3.5-turbo)

---

## Option 2: Ollama (FREE, Local, No API Key) üÜì

### Setup
1. Install Ollama: https://ollama.ai
2. Pull a model:
   ```bash
   ollama pull llama2        # or
   ollama pull mistral       # or
   ollama pull codellama
   ```
3. Start Ollama (runs automatically on install):
   ```bash
   ollama serve  # Usually runs automatically
   ```
4. (Optional) Configure if not using defaults:
   ```bash
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_MODEL=llama2  # or mistral, codellama, etc.
   ```

### Pros
- ‚úÖ **100% FREE** - No API key needed
- ‚úÖ Runs locally (privacy-friendly)
- ‚úÖ Works offline
- ‚úÖ Good personalization quality

### Cons
- ‚ö†Ô∏è Requires local installation (~4GB+ disk space)
- ‚ö†Ô∏è Slower than cloud APIs (runs on your machine)
- ‚ö†Ô∏è Needs good RAM (8GB+ recommended)

---

## Option 3: Gemini (Less Stable)

### Setup
```bash
GOOGLE_API_KEY=your-key-here
USE_GENAI=1
```

### Pros
- ‚úÖ Free tier available
- ‚úÖ Good quality

### Cons
- ‚ö†Ô∏è Less stable (as you mentioned)
- ‚ö†Ô∏è Rate limits

---

## Option 4: Local Transformers (Basic Fallback)

### Setup
```bash
USE_LOCAL_MODEL=1
```

### Pros
- ‚úÖ Completely offline
- ‚úÖ No API costs

### Cons
- ‚ö†Ô∏è Lower quality personalization
- ‚ö†Ô∏è Requires significant RAM
- ‚ö†Ô∏è Slower than other options

---

## Recommended Setup

**For Production (Stable):**
```bash
OPENAI_API_KEY=sk-your-key-here
OPENAI_MODEL=gpt-3.5-turbo
```

**For Development (Free):**
```bash
# Install Ollama first, then:
OLLAMA_MODEL=llama2
# No API key needed!
```

**For Maximum Reliability:**
Set up both OpenAI (primary) and Ollama (fallback):
```bash
OPENAI_API_KEY=sk-your-key-here
OLLAMA_MODEL=llama2
# System will use OpenAI, fallback to Ollama if OpenAI fails
```

---

## Testing

After setup, start the backend and check console output:
- ‚úÖ OpenAI configured - Model: gpt-3.5-turbo
- ‚úÖ Ollama configured - Base URL: http://localhost:11434, Model: llama2
- ‚úÖ Google Generative AI (Gemini) configured
- ‚úÖ Local text-generation model loaded

The first available provider will be used automatically.

---

## Personalization

**All providers** will generate personalized responses based on:
- User's age, weight, gender
- Health conditions
- Medication history (patient + family)
- Conversation history
- Sentiment analysis

Responses are **NOT hardcoded** - they're generated in real-time using the user's specific profile data.

